"""
APIæœåŠ¡ v2 - çº¯Promptæ–‡çŒ®ç»¼è¿°ç”Ÿæˆæ–¹æ¡ˆ
ä¸ä½¿ç”¨æ£€ç´¢APIï¼Œå®Œå…¨ä¾èµ–å¤§æ¨¡å‹çš„çŸ¥è¯†åº“
"""
import os
import json
import time
import asyncio
from typing import AsyncGenerator
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import signal
import sys

from config import Config
from llm_client import LLMClient
from review_generator_v2 import ReviewGeneratorV2
from prompt_template_v2 import detect_language


def load_env_file(env_file: str):
    """åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶"""
    if not os.path.isabs(env_file):
        current_dir = os.path.dirname(os.path.abspath(__file__))
        env_file = os.path.join(current_dir, env_file)
    
    if os.path.exists(env_file):
        print(f"âœ“ æ‰¾åˆ° .env æ–‡ä»¶: {env_file}")
        loaded_count = 0
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key] = value.strip('"\'')
                    loaded_count += 1
        print(f"âœ“ æˆåŠŸåŠ è½½ {loaded_count} ä¸ªç¯å¢ƒå˜é‡")
        return True
    else:
        print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_file}")
        return False


# åŠ è½½ç¯å¢ƒå˜é‡
load_env_file(".env")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="ICAIS2025-LiteratureReview API v2",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

@app.middleware("http")
async def simple_log_middleware(request, call_next):
    """ç®€åŒ–çš„æ—¥å¿—ä¸­é—´ä»¶"""
    start_time = time.time()
    path = request.url.path
    
    if not path.startswith("/health"):
        print(f"ğŸ“¥ [{time.strftime('%H:%M:%S')}] {request.method} {path}")
    
    try:
        response = await call_next(request)
        process_time = time.time() - start_time
        if not path.startswith("/health"):
            print(f"ğŸ“¤ [{time.strftime('%H:%M:%S')}] {request.method} {path} - {response.status_code} ({process_time:.3f}s)")
        return response
    except Exception as e:
        print(f"âŒ [{time.strftime('%H:%M:%S')}] é”™è¯¯: {request.method} {path} - {e}")
        raise

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# è®¾ç½®å…¨å±€è¶…æ—¶
REQUEST_TIMEOUT = Config.LITERATURE_REVIEW_TIMEOUT


class LiteratureReviewRequest(BaseModel):
    query: str


def format_sse_data(content: str) -> str:
    """ç”ŸæˆOpenAIæ ¼å¼çš„SSEæ•°æ®"""
    data = {
        "object": "chat.completion.chunk",
        "choices": [{
            "delta": {
                "content": content
            }
        }]
    }
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"


def format_sse_done() -> str:
    """ç”ŸæˆSSEç»“æŸæ ‡è®°"""
    return "data: [DONE]\n\n"


def stream_message(message: str, chunk_size: int = 1):
    """å°†æ¶ˆæ¯æŒ‰å­—ç¬¦æµå¼è¾“å‡ºï¼ˆåŒæ­¥ç”Ÿæˆå™¨ï¼‰"""
    for i in range(0, len(message), chunk_size):
        chunk = message[i:i + chunk_size]
        yield format_sse_data(chunk)


async def run_with_heartbeat(task_func, *args, heartbeat_interval=25, **kwargs):
    """
    æ‰§è¡Œé•¿æ—¶é—´ä»»åŠ¡ï¼ŒæœŸé—´å®šæœŸå‘é€å¿ƒè·³æ•°æ®
    
    Args:
        task_func: è¦æ‰§è¡Œçš„åŒæ­¥å‡½æ•°
        *args, **kwargs: ä¼ é€’ç»™å‡½æ•°çš„å‚æ•°
        heartbeat_interval: å¿ƒè·³é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤25ç§’
    
    Yields:
        å¿ƒè·³æ•°æ®ï¼ˆç©ºæ ¼å­—ç¬¦ï¼‰æˆ–ä»»åŠ¡ç»“æœ
    """
    start_time = time.time()
    last_heartbeat = start_time
    
    # åˆ›å»ºä»»åŠ¡ï¼ˆä½¿ç”¨asyncio.to_threadå°†åŒæ­¥å‡½æ•°è½¬æ¢ä¸ºåç¨‹ï¼‰
    task = asyncio.create_task(asyncio.to_thread(task_func, *args, **kwargs))
    
    # åœ¨ä»»åŠ¡æ‰§è¡ŒæœŸé—´å®šæœŸå‘é€å¿ƒè·³
    while not task.done():
        await asyncio.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
        elapsed = time.time() - last_heartbeat
        
        # å¦‚æœè¶…è¿‡å¿ƒè·³é—´éš”ï¼Œå‘é€å¿ƒè·³æ•°æ®
        if elapsed >= heartbeat_interval:
            yield format_sse_data(" ")  # å‘é€ä¸€ä¸ªç©ºæ ¼ä½œä¸ºå¿ƒè·³
            last_heartbeat = time.time()
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆ
        if task.done():
            break
    
    # ç­‰å¾…ä»»åŠ¡å®Œæˆå¹¶è¿”å›ç»“æœ
    try:
        result = await task
        # ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ¥åŒºåˆ†ç»“æœå’Œå¿ƒè·³æ•°æ®
        yield ("RESULT", result)
    except Exception as e:
        print(f"âš ï¸  ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(traceback.format_exc())
        raise e


async def _generate_review_internal(query: str) -> AsyncGenerator[str, None]:
    """å†…éƒ¨ç”Ÿæˆå™¨å‡½æ•°ï¼Œæ‰§è¡Œå®é™…çš„æ–‡çŒ®ç»¼è¿°ç”Ÿæˆé€»è¾‘ï¼ˆv2ç‰ˆæœ¬ï¼‰"""
    start_time = time.time()
    
    try:
        # å…ˆæ£€æµ‹è¯­è¨€ï¼Œç”¨äºåç»­æ¶ˆæ¯æ¨¡æ¿
        language = await asyncio.to_thread(detect_language, query)
        
        # æ ¹æ®è¯­è¨€è®¾ç½®æ¶ˆæ¯æ¨¡æ¿
        if language == 'zh':
            msg_templates = {
                'step1': "### ğŸ” æ­¥éª¤ 1/2: æŸ¥è¯¢ç†è§£ä¸çŸ¥è¯†è§„åˆ’\n\n",
                'step2': "### ğŸ“ æ­¥éª¤ 2/2: ç”Ÿæˆæ–‡çŒ®ç»¼è¿°\n\n",
                'final_title': "## ğŸ“„ æ–‡çŒ®ç»¼è¿°\n\n",
                'error_config': "## âŒ é”™è¯¯\n\né…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡è®¾ç½®\n\n",
                'error_config_exception': lambda e: f"## âŒ é”™è¯¯\n\né…ç½®éªŒè¯å¼‚å¸¸: {e}\n\n",
                'error_llm_init': lambda e: f"## âŒ é”™è¯¯\n\nLLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}\n\n",
                'error_timeout': lambda t: f"## âŒ è¶…æ—¶é”™è¯¯\n\nè¯·æ±‚å¤„ç†è¶…è¿‡ {t} ç§’ï¼Œå·²è‡ªåŠ¨ç»ˆæ­¢\n\n",
                'error_general': lambda e: f"## âŒ é”™è¯¯\n\nç¨‹åºæ‰§è¡Œå¤±è´¥: {e}\n\n"
            }
        else:
            msg_templates = {
                'step1': "### ğŸ” Step 1/2: Query Understanding and Knowledge Planning\n\n",
                'step2': "### ğŸ“ Step 2/2: Literature Review Generation\n\n",
                'final_title': "## ğŸ“„ Literature Review\n\n",
                'error_config': "## âŒ Error\n\nConfiguration validation failed. Please check environment variables.\n\n",
                'error_config_exception': lambda e: f"## âŒ Error\n\nConfiguration validation exception: {e}\n\n",
                'error_llm_init': lambda e: f"## âŒ Error\n\nLLM client initialization failed: {e}\n\n",
                'error_timeout': lambda t: f"## âŒ Timeout Error\n\nRequest processing exceeded {t} seconds. Automatically terminated.\n\n",
                'error_general': lambda e: f"## âŒ Error\n\nProcess execution failed: {e}\n\n"
            }
        
        # éªŒè¯é…ç½®ï¼ˆä¸è¾“å‡ºï¼‰
        try:
            config_valid = await asyncio.to_thread(Config.validate_config)
            if not config_valid:
                for chunk in stream_message(msg_templates['error_config']):
                    yield chunk
                return
        except Exception as e:
            for chunk in stream_message(msg_templates['error_config_exception'](e)):
                yield chunk
            return
        
        # åˆ›å»ºç»„ä»¶ï¼ˆä¸è¾“å‡ºåˆå§‹åŒ–ä¿¡æ¯ï¼‰
        try:
            llm_client = LLMClient()
        except Exception as e:
            for chunk in stream_message(msg_templates['error_llm_init'](e)):
                yield chunk
            return
        
        generator = ReviewGeneratorV2(llm_client, language=language)
        
        # æ­¥éª¤1: æŸ¥è¯¢ç†è§£ä¸çŸ¥è¯†è§„åˆ’
        for chunk in stream_message(msg_templates['step1']):
            yield chunk
        
        if language == 'zh':
            step1_progress = "ğŸ”„ æ­£åœ¨æ·±åº¦åˆ†ææŸ¥è¯¢æ„å›¾ï¼Œè§„åˆ’çŸ¥è¯†ç»“æ„...\n\n"
        else:
            step1_progress = "ğŸ”„ Deeply analyzing query intent, planning knowledge structure...\n\n"
        
        for chunk in stream_message(step1_progress):
            yield chunk
        
        knowledge_plan = None
        async for item in run_with_heartbeat(
            generator.understand_query,
            query,
            heartbeat_interval=25
        ):
            if isinstance(item, tuple) and len(item) == 2 and item[0] == "RESULT":
                knowledge_plan = item[1]
                break
            else:
                yield item
        
        if not knowledge_plan:
            if language == 'zh':
                error_msg = "## âŒ é”™è¯¯\n\næŸ¥è¯¢ç†è§£å¤±è´¥\n\n"
            else:
                error_msg = "## âŒ Error\n\nQuery understanding failed\n\n"
            for chunk in stream_message(error_msg):
                yield chunk
            return
        
        # æ­¥éª¤2: ç”Ÿæˆæ–‡çŒ®ç»¼è¿°
        for chunk in stream_message(msg_templates['step2']):
            yield chunk
        
        if language == 'zh':
            step2_progress = "ğŸ”„ æ­£åœ¨ç”Ÿæˆé«˜è´¨é‡æ–‡çŒ®ç»¼è¿°ï¼Œè¯·ç¨å€™...\n\n"
        else:
            step2_progress = "ğŸ”„ Generating high-quality literature review, please wait...\n\n"
        
        for chunk in stream_message(step2_progress):
            yield chunk
        
        review = None
        async for item in run_with_heartbeat(
            generator.generate_review,
            query, knowledge_plan,
            heartbeat_interval=25
        ):
            if isinstance(item, tuple) and len(item) == 2 and item[0] == "RESULT":
                review = item[1]
                break
            else:
                yield item
        
        # è¾“å‡ºæœ€ç»ˆç»¼è¿°
        if review:
            for chunk in stream_message(msg_templates['final_title']):
                yield chunk
            for chunk in stream_message(review):
                yield chunk
        else:
            if language == 'zh':
                error_msg = "## âŒ é”™è¯¯\n\næ–‡çŒ®ç»¼è¿°ç”Ÿæˆå¤±è´¥\n\n"
            else:
                error_msg = "## âŒ Error\n\nLiterature review generation failed\n\n"
            for chunk in stream_message(error_msg):
                yield chunk
        
    except asyncio.TimeoutError:
        elapsed = time.time() - start_time
        if language == 'zh':
            error_msg = f"## âŒ è¶…æ—¶é”™è¯¯\n\nè¯·æ±‚å¤„ç†è¶…è¿‡ {REQUEST_TIMEOUT} ç§’ï¼Œå·²è‡ªåŠ¨ç»ˆæ­¢\n\n"
        else:
            error_msg = f"## âŒ Timeout Error\n\nRequest processing exceeded {REQUEST_TIMEOUT} seconds. Automatically terminated.\n\n"
        for chunk in stream_message(error_msg):
            yield chunk
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ–‡çŒ®ç»¼è¿°æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        print(traceback.format_exc())
        if language == 'zh':
            error_msg = f"## âŒ é”™è¯¯\n\nç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}\n\n"
        else:
            error_msg = f"## âŒ Error\n\nProcess execution failed: {str(e)}\n\n"
        for chunk in stream_message(error_msg):
            yield chunk
    finally:
        yield format_sse_done()


@app.post("/literature_review", response_class=StreamingResponse)
async def literature_review_endpoint(request: LiteratureReviewRequest):
    """
    æ–‡çŒ®ç»¼è¿°ç”Ÿæˆç«¯ç‚¹ï¼ˆv2ç‰ˆæœ¬ - çº¯Promptæ–¹æ¡ˆï¼‰
    
    ä¸ä½¿ç”¨æ£€ç´¢APIï¼Œå®Œå…¨ä¾èµ–å¤§æ¨¡å‹çš„çŸ¥è¯†åº“ç”Ÿæˆé«˜è´¨é‡æ–‡çŒ®ç»¼è¿°
    """
    try:
        # ä½¿ç”¨asyncio.wait_forè®¾ç½®è¶…æ—¶
        async def generate_with_timeout():
            async for chunk in _generate_review_internal(request.query):
                yield chunk
        
        return StreamingResponse(
            generate_with_timeout(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
    except Exception as e:
        print(f"âŒ å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "ok", "service": "ICAIS2025-LiteratureReview API v2", "version": "2.0.0"}


# ä¿¡å·å¤„ç†
def signal_handler(sig, frame):
    """å¤„ç†é€€å‡ºä¿¡å·"""
    print("\næ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡...")
    sys.exit(0)


signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)


if __name__ == "__main__":
    import uvicorn
    
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "3000"))
    
    print(f"ğŸš€ å¯åŠ¨ ICAIS2025-LiteratureReview API v2 æœåŠ¡...")
    print(f"ğŸ“ æœåŠ¡åœ°å€: http://{host}:{port}")
    print(f"ğŸ“š APIæ–‡æ¡£: http://{host}:{port}/docs")
    
    uvicorn.run(
        "api_service_v2:app",
        host=host,
        port=port,
        log_level="info",
        reload=False
    )

